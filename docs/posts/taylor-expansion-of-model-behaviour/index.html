<!DOCTYPE html>
<html lang="en" dir="auto">

<head><script src="/livereload.js?mindelay=10&amp;v=2&amp;port=1313&amp;path=livereload" data-no-instant defer></script><meta charset="utf-8">
<meta http-equiv="X-UA-Compatible" content="IE=edge">
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<meta name="robots" content="noindex, nofollow">
<title>Taylor expansion of Model Behaviour | Prud&#39;s Log</title>
<meta name="keywords" content="math, machine-learning, statistics">
<meta name="description" content="Model behaviour changes maybe dependent on Hessian, Learning Rate and Weight Norms">
<meta name="author" content="Prudhviraj Naidu">
<link rel="canonical" href="http://localhost:1313/posts/taylor-expansion-of-model-behaviour/">
<link crossorigin="anonymous" href="/assets/css/stylesheet.e7df42763d8de318466c4e083e726ef1427564ee027449f8754ebebe290054cc.css" integrity="sha256-599Cdj2N4xhGbE4IPnJu8UJ1ZO4CdEn4dU6&#43;vikAVMw=" rel="preload stylesheet" as="style">
<link rel="icon" href="http://localhost:1313/favicon.ico">
<link rel="icon" type="image/png" sizes="16x16" href="http://localhost:1313/favicon-16x16.png">
<link rel="icon" type="image/png" sizes="32x32" href="http://localhost:1313/favicon-32x32.png">
<link rel="apple-touch-icon" href="http://localhost:1313/apple-touch-icon.png">
<link rel="mask-icon" href="http://localhost:1313/safari-pinned-tab.svg">
<meta name="theme-color" content="#2e2e33">
<meta name="msapplication-TileColor" content="#2e2e33">
<link rel="alternate" hreflang="en" href="http://localhost:1313/posts/taylor-expansion-of-model-behaviour/">
<noscript>
    <style>
        #theme-toggle,
        .top-link {
            display: none;
        }

    </style>
    <style>
        @media (prefers-color-scheme: dark) {
            :root {
                --theme: rgb(29, 30, 32);
                --entry: rgb(46, 46, 51);
                --primary: rgb(218, 218, 219);
                --secondary: rgb(155, 156, 157);
                --tertiary: rgb(65, 66, 68);
                --content: rgb(196, 196, 197);
                --code-block-bg: rgb(46, 46, 51);
                --code-bg: rgb(55, 56, 62);
                --border: rgb(51, 51, 51);
            }

            .list {
                background: var(--theme);
            }

            .list:not(.dark)::-webkit-scrollbar-track {
                background: 0 0;
            }

            .list:not(.dark)::-webkit-scrollbar-thumb {
                border-color: var(--theme);
            }
        }

    </style>
</noscript>
<script>
MathJax = {
  tex: {
    inlineMath: [['$', '$'], ['\\(', '\\)']],
    displayMath: [['$$', '$$'], ['\\[', '\\]']],
    processEscapes: true,
    processEnvironments: true,
    packages: {'[+]': ['ams', 'newcommand', 'configmacros']}
  },
  options: {
    skipHtmlTags: ['script', 'noscript', 'style', 'textarea', 'pre']
  }
};

window.addEventListener('load', (event) => {
  document.querySelectorAll("mjx-container").forEach(function(x){
    x.parentElement.classList += 'has-jax'})
});
</script>
<script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
<script type="text/javascript" id="MathJax-script" async
  src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
</head>

<body class="" id="top">
<script>
    
    document.body.classList.remove('dark');
    
    localStorage.removeItem("pref-theme");
</script>

<header class="header">
    <nav class="nav">
        <div class="logo">
            <a href="http://localhost:1313/" accesskey="h" title="Prud&#39;s Log (Alt + H)">Prud&#39;s Log</a>
            <div class="logo-switches">
            </div>
        </div>
        <ul id="menu">
            <li>
                <a href="http://localhost:1313/" title="Home">
                    <span>Home</span>
                </a>
            </li>
            <li>
                <a href="http://localhost:1313/posts/" title="Blog">
                    <span>Blog</span>
                </a>
            </li>
            <li>
                <a href="http://localhost:1313/cv/" title="CV">
                    <span>CV</span>
                </a>
            </li>
        </ul>
    </nav>
</header>
<main class="main">

<article class="post-single">
  <header class="post-header">
    
    <h1 class="post-title entry-hint-parent">
      Taylor expansion of Model Behaviour
    </h1>
    <div class="post-meta"><span title='2025-07-06 15:51:49 -0700 PDT'>July 6, 2025</span>&nbsp;·&nbsp;Prudhviraj Naidu

</div>
  </header> 
  <div class="post-content"><p><em>Warning: First draft, read at your own peril</em></p>
<h2 id="background">Background<a hidden class="anchor" aria-hidden="true" href="#background">#</a></h2>
<p>Language models are probabilistic models $P_{\theta}(x)$ where $x$ is a string and $\theta$ is the model parameters. During gradient descent, we update $\theta$ and calculate loss (or error function) $L(\theta)$.</p>
<p>In this article, I want to understand how the model changes when we change $\theta$ at timestep $t+1$ w.r.t the previous timestep $\theta_t$. This article is partially motivated by Natural Gradients<sup id="fnref:1"><a href="#fn:1" class="footnote-ref" role="doc-noteref">1</a></sup> and partly because I am driven by nightmares that I am not setting the learning rate correctly! xD</p>
<p>These derivations were shown by Andy<sup id="fnref1:1"><a href="#fn:1" class="footnote-ref" role="doc-noteref">1</a></sup>. I just want to re-write them for my understanding.</p>
<h2 id="the-first-two-terms-are-zero">The first two terms are zero<a hidden class="anchor" aria-hidden="true" href="#the-first-two-terms-are-zero">#</a></h2>
<p>Let $P_{\theta}$ refer to the probability measure induced by $\theta$ on some set of strings $X$.</p>
<div>
$$
\begin{aligned}
D_{KL}(P_{\theta_t} || P_{\theta}) &\approx D_{KL}(P_{\theta_t} || P_{\theta}) \\
&\quad + (\nabla_{\theta} D_{KL}(P_{\theta_t} || P_{\theta}) \left. \right|_{\theta=\theta_t}) (\theta - \theta_t) \\
&\quad + (\theta - \theta_t)^{\top} H(\theta_t) (\theta - \theta_t)
\end{aligned}
$$
</div>
<p>where H is the Hessian w.r.t $\theta$.</p>
<p>The first term $D_{KL}(P_{\theta_t} || P_{\theta_t})$ is $0$ as the probability distributions are the same.</p>
<p>The second term is also $0$. This is quite interesting.</p>
<!-- <div>
$$
\begin{aligned}
\nabla_{\theta} D_{KL}(P_{\theta}, P_{\theta_t}) \left. \right|_{\theta=\theta_t} &= \nabla_{\theta}\ \mathbb{E}_{x \sim P_{\theta_t}} \left[\log \frac{P_{\theta} (x)}{P_{\theta_t} (x)} \right] \\
\text{ Since the $x$ is drawn from $P_{\theta_t}$ which does not depend on $\theta$} \\
&= \mathbb{E}_{x \sim P_{\theta_t}} \nabla_{\theta}\ \left[\log \frac{P_{\theta} (x)}{P_{\theta_t} (x)} \right] \\
&= \mathbb{E}_{x \sim P_{\theta_t}} \nabla_{\theta}\ \left[\log P_{\theta} (x) - \log P_{\theta_t} (x) \right] \\
\text{Since $P_{\theta_t} (x)$ does on depend on $\theta$} \\
&= \mathbb{E}_{x \sim P_{\theta_t}} \nabla_{\theta}\ \left[\log P_{\theta} (x) \right] \left. \right|_{\theta=\theta_t} \\ 
\text{As we are evaluating the gradient at $\theta=\theta_t$ } \\
&= \mathbb{E}_{x \sim P_{\theta_t}} \frac{1}{P_{\theta_t} (x)} \nabla_{\theta}\ P_{\theta} (x) \left. \right|_{\theta=\theta_t}\\
&= \sum_{x} P_{\theta_t} (x) \frac{1}{P_{\theta_t} (x)} \nabla_{\theta}\ P_{\theta} (x) \left. \right|_{\theta=\theta_t}\\
&= \sum_{x} \nabla_{\theta}\ P_{\theta} (x) \left. \right|_{\theta=\theta_t}\\
\text{By the magic of linearity} \\
&= \nabla_{\theta}\ \sum_{x} P_{\theta} (x) \left. \right|_{\theta=\theta_t}\\
&= \nabla_{\theta}\ 1 \\
&= 0 \\
\end{aligned}
$$
</div> -->
<div>
$$
\begin{aligned}
\nabla_{\theta} D_{KL}(P_{\theta_t} || P_{\theta}) \left. \right|_{\theta=\theta_t} &= \nabla_{\theta}\ \mathbb{E}_{x \sim P_{\theta_t}} \left[\log \frac{P_{\theta_t} (x)}{P_{\theta} (x)} \right] \left. \right|_{\theta=\theta_t} \\
\text{ Since the $x$ is drawn from $P_{\theta_t}$ which does not depend on $\theta$} \\
&= \mathbb{E}_{x \sim P_{\theta_t}} \nabla_{\theta}\ \left[\log \frac{P_{\theta_t} (x)}{P_{\theta} (x)} \right] \left. \right|_{\theta=\theta_t} \\
&= \mathbb{E}_{x \sim P_{\theta_t}} \nabla_{\theta}\ \left[\log P_{\theta_t} (x) - \log P_{\theta} (x) \right] \left. \right|_{\theta=\theta_t} \\
\text{Since $P_{\theta_t} (x)$ does not depend on $\theta$} \\
&= \mathbb{E}_{x \sim P_{\theta_t}} \left[- \nabla_{\theta}\ \log P_{\theta} (x) \right] \left. \right|_{\theta=\theta_t} \\ 
&= -\mathbb{E}_{x \sim P_{\theta_t}} \left[\nabla_{\theta}\ \log P_{\theta} (x) \right] \left. \right|_{\theta=\theta_t} \\ 
\text{As we are evaluating the gradient at $\theta=\theta_t$ } \\
&= -\mathbb{E}_{x \sim P_{\theta_t}} \frac{1}{P_{\theta_t} (x)} \nabla_{\theta}\ P_{\theta} (x) \left. \right|_{\theta=\theta_t}\\
&= -\sum_{x} P_{\theta_t} (x) \frac{1}{P_{\theta_t} (x)} \nabla_{\theta}\ P_{\theta} (x) \left. \right|_{\theta=\theta_t}\\
&= -\sum_{x} \nabla_{\theta}\ P_{\theta} (x) \left. \right|_{\theta=\theta_t}\\
\text{By the magic of linearity} \\
&= -\nabla_{\theta}\ \sum_{x} P_{\theta} (x) \left. \right|_{\theta=\theta_t}\\
&= -\nabla_{\theta}\ 1 \\
&= 0 \\
\end{aligned}
$$
</div>
<p>This means $D_{KL}(P_{\theta_t} || P_{\theta}) \approx (\theta - \theta_t)^{\top} H(\theta_t) (\theta - \theta_t)$.</p>
<h2 id="bounded-weight-updates-bound-behaviour">Bounded weight updates bound behaviour<a hidden class="anchor" aria-hidden="true" href="#bounded-weight-updates-bound-behaviour">#</a></h2>
<p>Let&rsquo;s ask the question what is the maximum $D_{KL}(P_{\theta_t} || P_{\theta})$ given finite weight updates $\Delta \theta$.</p>
<div>
$$
\begin{aligned}
D_{KL}(P_{\theta_t} || P_{\theta}) &\approx \frac{1}{2}(\Delta \theta)^{\top} H(\theta_t) (\Delta \theta) \\
&\le \frac{1}{2}(\Delta \theta)^{\top} \lambda_{max}(\theta_t) I (\Delta \theta) \\
&\le \frac{1}{2}\lambda_{max}(\theta_t) (\Delta \theta)^{\top} (\Delta \theta) \\
&\le \frac{1}{2}\lambda_{max}(\theta_t) \| \Delta \theta \|^2_2 \\
\end{aligned}
$$
</div>
<p>where $\lambda_{max}(\theta_t)$ is the largest eigenvalue of $H(\theta_t)$</p>
<p>If we consider a weight update in SGD with l2 regularization:</p>
<div>
$$
\theta_{t+1} = \theta_{t} - \nu_t (g_t + \gamma \theta_t)
$$
</div>
<p>where $\nu_t$ is the step multiplier (or learning rate at step t) and $\gamma$ is the weight decay coefficient. Furthermore, let&rsquo;s presume that the gradient norm is clipped to 1 which is a standard practice.</p>
<div>
$$
\begin{aligned}
D_{KL}(P_{\theta_t} || P_{\theta_{t+1}}) &\le \frac{1}{2}\lambda_{max}(\theta_t) \| \Delta \theta \|^2_2 \\
&\le \frac{1}{2}\lambda_{max}(\theta_t) \| \nu_t (g_t + \gamma \theta_t) \|^2_2 \\
&\le \frac{1}{2}\lambda_{max}(\theta_t) \nu_t^2 \| (g_t + \gamma \theta_t) \|^2_2 \\
&\le \frac{1}{2}\lambda_{max}(\theta_t) \nu_t^2 \left( \| g_t \|_2 + \| \gamma \theta_t \|_2 \right)^2 \\
&\le \frac{1}{2}\lambda_{max}(\theta_t) \nu_t^2 \left( 1 + \gamma \| \theta_t \|_2 \right)^2 \\
\end{aligned}
$$
</div>
<p>We can conclude three things from the bound $D_{KL}(P_{\theta_t} || P_{\theta_{t + 1}})$:</p>
<ol>
<li>
<p>The eigenvalues of the Hessian influence model behaviour.</p>
</li>
<li>
<p>Weight regularization only increases the magnitude of KL-divergence.</p>
</li>
<li>
<p>Smaller learning rate limits change in model behaviour in accordance with expectation.</p>
</li>
</ol>
<p>It would be interesting to see how problem formulation or model architecture can change the Hessian. This may suggest certain training methodologies more readily change model behaviours.</p>
<p>In fine-tuning, practioners use small learning rates. Despite this, the model behaviours seems to change drastically. This maybe because of we measure other metrics rather than KL-divergence but it may also hint that the eigenvalues of the Hessian are large.</p>
<p>Ghorbani et al, 2019<sup id="fnref:2"><a href="#fn:2" class="footnote-ref" role="doc-noteref">2</a></sup> shows the eigenvalues of the Hessian getting larger during training. They demonstrated this for ResNet over image datasets.</p>
<p>We could view pre-training through the lens of eigenstructure of the hessian. Pre-training neural networks helps with finetuning. Measure the PSD-ness of the Hessian could serve as an indicator of how much the behaviour could change during fine-tuning (even RL-finetuning).</p>
<h3 id="footnotes">Footnotes<a hidden class="anchor" aria-hidden="true" href="#footnotes">#</a></h3>
<div class="footnotes" role="doc-endnotes">
<hr>
<ol>
<li id="fn:1">
<p><a href="https://andrewcharlesjones.github.io/journal/natural-gradients.html">Andy Jones. &ldquo;Natural gradients&rdquo;</a>&#160;<a href="#fnref:1" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a>&#160;<a href="#fnref1:1" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:2">
<p><a href="https://proceedings.mlr.press/v97/ghorbani19b.html">Behrooz Ghorbani, Shankar Krishnan, Ying Xiao. ICML 2019. &ldquo;An Investigation into Neural Net Optimization via Hessian Eigenvalue Density&rdquo;</a>&#160;<a href="#fnref:2" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
</ol>
</div>


  </div>

  <footer class="post-footer">
    <ul class="post-tags">
      <li><a href="http://localhost:1313/tags/math/">Math</a></li>
      <li><a href="http://localhost:1313/tags/machine-learning/">Machine-Learning</a></li>
      <li><a href="http://localhost:1313/tags/statistics/">Statistics</a></li>
    </ul>
  </footer>
</article>
    </main>
    
<footer class="footer">
    <span>&copy; 2026 <a href="http://localhost:1313/">Prud&#39;s Log</a></span>
    <span> · Inspired by <a href="https://lilianweng.github.io/" rel="noopener noreferrer" target="_blank">Lil'Log</a></span>
    <span> · 
        Powered by
        <a href="https://gohugo.io/" rel="noopener noreferrer" target="_blank">Hugo</a> &
        <a href="https://github.com/adityatelange/hugo-PaperMod/" rel="noopener" target="_blank">PaperMod</a>
    </span>
</footer></body>

</html>
